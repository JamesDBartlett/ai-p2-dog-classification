{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": " P2 - Dog Breed Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7qTN4aS_MxfO",
        "gquoXjtJJdDb",
        "TqU_v8bHJdDl",
        "2xKXiYqDJdEB",
        "qC2VtQVJJdEc",
        "ia1ph_KnJdE5",
        "BJcVZKxsJdFS",
        "uen1zKE4JdFW"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamesDBartlett/ai-p2-dog-classification/blob/master/P2_Dog_Breed_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lgF-7w5JmOx",
        "colab_type": "text"
      },
      "source": [
        "# **P2 - Dog Breed Classification with Convolutional Neural Networks**\n",
        "## James D. Bartlett III\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSsQiv7U5tIH",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "escUZF66Lq5y",
        "colab_type": "text"
      },
      "source": [
        "### Table of Contents\n",
        "\n",
        "* [Step 00](#step00): Download Datasets & Models\n",
        "* [Step 0](#step0): Import Datasets\n",
        "* [Step 1](#step1): Detect Humans\n",
        "* [Step 2](#step2): Detect Dogs\n",
        "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
        "* [Step 4](#step4): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
        "* [Step 5](#step5): Write your Algorithm\n",
        "* [Step 6](#step6): Test Your Algorithm\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qTN4aS_MxfO",
        "colab_type": "text"
      },
      "source": [
        "<a id='step00'></a>\n",
        "## Step 00: Download Datasets & Models\n",
        "\n",
        "**Download & unpack all necessary files.**\n",
        "\n",
        "_I used Python to perform all of the file operations here, so that this notebook will run without modifications on any operating system._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjG_57v88u6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Beginning Step 00: \"Download Datasets\"')\n",
        "print('-------------------------------------')\n",
        "\n",
        "# Install Python packages\n",
        "!pip install gitpython ImageScraper\n",
        "\n",
        "# Import Python modules\n",
        "import os\n",
        "import cv2\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import shutil\n",
        "import random\n",
        "import datetime\n",
        "import numpy as np\n",
        "from os import path\n",
        "import urllib.request\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "from zipfile import ZipFile\n",
        "from IPython import display\n",
        "from git.repo.base import Repo\n",
        "import matplotlib.pyplot as plt \n",
        "from collections import OrderedDict\n",
        "\n",
        "# change matplotlib to inline mode\n",
        "%matplotlib inline\n",
        "\n",
        "# Import PyTorch modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "import torchvision.models as models\n",
        "from torch.optim import lr_scheduler \n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# check if cuda is available\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "# define deleteIfExists function, which takes a directory as its argument\n",
        "def deleteIfExists(f):\n",
        "  if(path.exists(f)):\n",
        "    shutil.rmtree(f)\n",
        "\n",
        "# define horizBar function, which takes char(string) and x(integer) as its arguments\n",
        "# concatenates char x times, and prints in a single row, generating a horizontal bar across the page\n",
        "def horizBar(char, x):\n",
        "  print(char * x)\n",
        "\n",
        "# define current_time function, which prints the current time in GMT\n",
        "def current_time():\n",
        "  horizBar('\\\\', 84)\n",
        "  print(\"Current Time: \" + time.strftime(\"%H:%M:%S\", time.gmtime()) + \" (GMT)\")\n",
        "  horizBar('/', 84)\n",
        "\n",
        "# define floatingZeros function, which takes n(integer) as its argument\n",
        "# and returns an array of n floating-point zeros\n",
        "def floatingZeros(n):\n",
        "  return [x.item() for x in np.zeros(n, dtype=float)]\n",
        "\n",
        "# Handle different operating systems' directory delimiter character\n",
        "ddl = \"\"\n",
        "if(sys.platform in (\"win32\", \"win64\", \"cygwin\")):\n",
        "  ddl = \"\\\\\"\n",
        "  print(\"Windows operating system detected. Setting directory delimiter to back-slash.\")\n",
        "else:\n",
        "  print(\"Non-Windows operating system detected. Setting directory delimiter to forward-slash.\")\n",
        "  ddl = \"/\"\n",
        "\n",
        "# declare \"slashterisk\" string for simplified recursive directory traversal\n",
        "slshtrsk = ddl + \"*\" # Example: `\"lfw\" + slshtrsk * 3` evaluates to `lfw/*/*/*`\n",
        "\n",
        "# define printLosses function, which takes 2 arguments: train_loss and valid_loss\n",
        "def printLosses(train_loss, valid_loss):\n",
        "  print(f\"Training Loss: {train_loss:.6f} \\tValidation Loss: {valid_loss:.6f}\")\n",
        "\n",
        "# define printAccuracies function, which takes 2 arguments: train_acc and valid_acc\n",
        "def printAccuracies(train_acc, valid_acc):\n",
        "  print(f\"Training Accuracy: {train_acc:.6f} \\tValidation Accuracy: {valid_acc:.6f}\")\n",
        "\n",
        "# define printEpochTime function, which takes 1 argument: time (in seconds)\n",
        "def printEpochTime(time):\n",
        "  print(f\"Epoch Time: {(time // 60):.0f}m {(time % 60):.0f}s\")\n",
        "\n",
        "# define validLossEval function with 4 arguments: v_loss_min, v_loss, model, and model_file_path\n",
        "def validLossEval(v_loss_min, v_loss, model, model_file_path):\n",
        "  # print the current validation loss and previous minimum\n",
        "  print(f\"Previous minimum Validation Loss: {v_loss_min:.6f}\")\n",
        "  # if validation loss improved, print message, save model, and return new minimum value\n",
        "  if v_loss_min > v_loss:\n",
        "    print(\"This epoch produced the BEST (lowest) validation loss so far! Saving the model...\")\n",
        "    torch.save(model.state_dict(), model_file_path)\n",
        "    print(\"Proceeding to next epoch...\")\n",
        "    return v_loss\n",
        "  # otherwise, print message, do NOT save model, and return previous minimum value\n",
        "  else:\n",
        "    print(\"This epoch did NOT produce an improved validation loss. Proceeding to next epoch...\")\n",
        "    return v_loss_min\n",
        "\n",
        "# define printTestResults function, which takes 3 arguments: test_loss, correct, and total\n",
        "# print test loss as floating point value and accuracy as percentage\n",
        "def printTestResults(test_loss, correct, total):\n",
        "  pct = 100 * correct / total\n",
        "  print(\"Test Loss: %6f \\tTest Accuracy: %.2f%% (%2d/%2d)\" % (test_loss, pct, correct, total))\n",
        "\n",
        "\n",
        "# if haarcascades xml file does not exist locally, get it from GitHub\n",
        "haarcascades_path = 'haarcascades' + ddl + 'haarcascade_frontalface_alt.xml'\n",
        "if not path.exists(haarcascades_path):\n",
        "  os.mkdir('haarcascades')\n",
        "  urllib.request.urlretrieve(\\\n",
        "    'https://raw.githubusercontent.com/JamesDBartlett/ai-p2-dog-classification/master/haarcascades/haarcascade_frontalface_alt.xml',\\\n",
        "    haarcascades_path)\n",
        "\n",
        "# Clone convolution helper functions gist\n",
        "convHelperFunctionsURL = \"https://gist.github.com/a4398d7cf7ba984f031433c769ddba5c.git\"\n",
        "deleteIfExists(\"conv-helper-functions\")\n",
        "print(\"Cloning & importing 'Pytorch Conv2d: Helper Functions for Output Shape & Padding' gist from GitHub...\")\n",
        "Repo.clone_from(convHelperFunctionsURL, \"conv-helper-functions\")\n",
        "sys.path.append(os.path.abspath(\"conv-helper-functions\"))\n",
        "import conv_output as cnvOut\n",
        "print(\"Done.\")\n",
        "\n",
        "# Define models\n",
        "VGG16 = models.vgg16(pretrained = True)\n",
        "densenet161 = models.densenet161(pretrained = True)\n",
        "# resnext101 = models.resnext101_32x8d(pretrained = True)\n",
        "# wide_resnet50 = models.wide_resnet50_2(pretrained = True)\n",
        "# wide_resnet101 = models.wide_resnet101_2(pretrained = True)\n",
        "\n",
        "# tell PIL to go ahead and load truncated images\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcEqx8X-Lvui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declare filesToDownload array\n",
        "filesToDownload = [\"https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\",\n",
        "                   \"https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip\"]\n",
        "\n",
        "# Declare zippedFiles array\n",
        "zippedFiles = []\n",
        "\n",
        "# Download files and populate zippedFiles array with their filenames\n",
        "print(\"\\nAcquiring datasets from AWS, or using local copies if available.\")\n",
        "for i in filesToDownload:\n",
        "  # Extract the filename from the URL\n",
        "  name = i.rsplit('/', 1)[-1]\n",
        "  if not (path.exists(path.splitext(name)[0])):\n",
        "    print(\" - Downloading file: \" + \"'\" + name + \"' ...\")\n",
        "    urllib.request.urlretrieve(i, name)\n",
        "    # Add filename to zippedFiles array\n",
        "    zippedFiles.append(name)\n",
        "    print(\" -- '\" + name + \"'\" + \" downloaded successfully.\")\n",
        "print(\"Dataset acquisition complete.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bAEtgfZzm7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Delete previously-unzipped folders if they exist, extract & delete zip files, leaving freshly-unzipped folders\n",
        "print(\"\\nUnpacking zip files...\")\n",
        "for i in zippedFiles:\n",
        "  # Extract target folder name from zipped filename\n",
        "  targetFolder = path.splitext(i)[0]\n",
        "  deleteIfExists(targetFolder)\n",
        "  print(\" - Unpacking file: \" + \"'\" + i + \"' ...\")\n",
        "  with ZipFile(i, 'r') as zipObj:\n",
        "    zipObj.extractall(targetFolder)\n",
        "  print(\" -- '\" + i + \"'\" + \" unpacked. Deleting zip file...\")\n",
        "  os.remove(i)\n",
        "  deleteIfExists(targetFolder + ddl + \"__MACOSX\")\n",
        "  print(\" -- '\" + i + \"'\" + \" deleted.\")\n",
        "print(\"All zip files unpacked and deleted.\")\n",
        "print(\"\\nStep 00 Complete. Proceeding to Step 0...\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gquoXjtJJdDb",
        "colab_type": "text"
      },
      "source": [
        "<a id='step0'></a>\n",
        "## Step 0: Import Datasets\n",
        "\n",
        "In the code cell below, we save the file paths for both the human (LFW) dataset and dog dataset in the numpy arrays `human_files` and `dog_files`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4SWGaZfJdDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load filenames for human and dog images\n",
        "human_pix = np.array(glob(\"lfw\" + slshtrsk * 3))\n",
        "dog_pix = np.array(glob(\"dogImages\" + slshtrsk * 4))\n",
        "\n",
        "# print number of images in each dataset\n",
        "print(\"There are %d total human images.\" % len(human_pix))\n",
        "print(\"There are %d total dog images.\" % len(dog_pix))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqU_v8bHJdDl",
        "colab_type": "text"
      },
      "source": [
        "<a id='step1'></a>\n",
        "## Step 1: Detect Humans\n",
        "\n",
        "In this section, we use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  \n",
        "\n",
        "OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.  In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3gxObe2JdDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract pre-trained face detector\n",
        "face_cascade = cv2.CascadeClassifier(haarcascades_path)\n",
        "\n",
        "# load color (BGR) image\n",
        "pic = cv2.imread(human_pix[0])\n",
        "# convert BGR image to grayscale\n",
        "gray = cv2.cvtColor(pic, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# find faces in image\n",
        "faces = face_cascade.detectMultiScale(gray)\n",
        "\n",
        "# print number of faces detected in the image\n",
        "print('Number of faces detected:', len(faces))\n",
        "\n",
        "# get bounding box for each detected face\n",
        "for (x, y, w, h) in faces:\n",
        "  # add bounding box to color image\n",
        "  cv2.rectangle(pic, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "    \n",
        "# convert BGR image to RGB for plotting\n",
        "cv_rgb = cv2.cvtColor(pic, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# display the image, along with bounding box\n",
        "plt.imshow(cv_rgb)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdM_CWb0JdDu",
        "colab_type": "text"
      },
      "source": [
        "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
        "\n",
        "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
        "\n",
        "### Write a Human Face Detector\n",
        "\n",
        "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "395VyoR3JdDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define human_face_detector function, which returns true if a human face is detected\n",
        "def human_face_detector(pic_path):\n",
        "  return len(\n",
        "      face_cascade.detectMultiScale(\n",
        "        cv2.cvtColor(\n",
        "            cv2.imread(pic_path), cv2.COLOR_BGR2GRAY\n",
        "            ))) > 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDTb0WhbJdDz",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Assess the Human Face Detector\n",
        "\n",
        "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
        "- What percentage of the first 100 images in `human_files` have a detected human face?\n",
        "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
        "\n",
        "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR67goZyJdDz",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "__Answer:__ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3S8ivbdJdD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get first 100 images from each image set\n",
        "human_pix_short = human_pix[:100]\n",
        "dog_pix_short = dog_pix[:100]\n",
        "\n",
        "# define a function called detect that takes a set of images as input,\n",
        "# and returns the count of how many contained a human face\n",
        "def detect(images):\n",
        "  count = 0\n",
        "  for i in tqdm(images):\n",
        "    if human_face_detector(i):\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "# run the detect function on human_pix_short and dog_pix_short, and print the results\n",
        "print(\"\\nHuman face(s) detected in the first 100 images from: \\nhuman_pix: {} \\tdog_pix: {}\".format(\n",
        "  detect(human_pix_short), detect(dog_pix_short)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xKXiYqDJdEB",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<a id='step2'></a>\n",
        "## Step 2: Detect Dogs\n",
        "\n",
        "In this section, we use a [pre-trained model](http://pytorch.org/docs/master/torchvision/models.html) to detect dogs in images.  \n",
        "\n",
        "### Obtain Pre-trained VGG-16 Model\n",
        "\n",
        "The code cell below downloads the VGG-16 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfLeNN7NJdED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# move VGG16 to cuda if available\n",
        "if use_cuda:\n",
        "  VGG16 = VGG16.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_U_uDAmJdEI",
        "colab_type": "text"
      },
      "source": [
        "Given an image, this pre-trained VGG-16 model returns a prediction (derived from the 1000 possible categories in ImageNet) for the object that is contained in the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SCUUDqpJdEI",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Making Predictions with a Pre-trained Model\n",
        "\n",
        "In the next code cell, you will write a function that accepts a path to an image (such as `'dogImages/train/001.Affenpinscher/Affenpinscher_00001.jpg'`) as input and returns the index corresponding to the ImageNet class that is predicted by the pre-trained VGG-16 model.  The output should always be an integer between 0 and 999, inclusive.\n",
        "\n",
        "Before writing the function, make sure that you take the time to learn  how to appropriately pre-process tensors for pre-trained models in the [PyTorch documentation](http://pytorch.org/docs/stable/torchvision/models.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEULfPZJJdEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preProcessPic(i):\n",
        "  # open incoming image with PIL\n",
        "  pic = Image.open(i)\n",
        "\n",
        "  # find image height & width\n",
        "  width = pic.size[0]\n",
        "  height = pic.size[1] \n",
        "\n",
        "  # resize image using PIL's Image.thumbnail method with the LANCZOS resampling filter option\n",
        "  x, y = (256, 1000) if width > height else (1000, 256)\n",
        "  pic.thumbnail([x, y], Image.LANCZOS)\n",
        "  \n",
        "  # set image margins\n",
        "  left = 0.5 * (pic.width - 224)\n",
        "  bottom = 0.5 * (pic.height - 224)\n",
        "  right = left + 224\n",
        "  top = bottom + 224\n",
        "\n",
        "  # crop image\n",
        "  _ = pic.crop((left, bottom, right, top))\n",
        "\n",
        "  # normalize, transpose, and return\n",
        "  pic_array = np.array(pic) / 255\n",
        "  return ((pic_array - np.mean(pic_array)) / np.std(pic_array)).transpose(2, 0, 1)\n",
        "\n",
        "def VGG16_predict(pic_path):\n",
        "  # preprocess incoming image\n",
        "  pic = preProcessPic(pic_path)\n",
        "\n",
        "  # convert to tensor\n",
        "  pic = torch.from_numpy(pic).type(torch.FloatTensor)\n",
        "  pic.unsqueeze_(0)\n",
        "\n",
        "  # load image into cuda if available\n",
        "  if use_cuda:\n",
        "    pic = pic.cuda()\n",
        "\n",
        "  # predict label\n",
        "  probs = torch.exp(VGG16.forward(pic)) \n",
        "  _, label = probs.topk(1)     \n",
        "  \n",
        "  # return index of predicted class\n",
        "  return label "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx0z4IaNJdEN",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Write a Dog Detector\n",
        "\n",
        "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained VGG-16 model, we need only check if the pre-trained model predicts an index between 151 and 268 (inclusive).\n",
        "\n",
        "Use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN0vUvCeJdEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define dog_detector function, which returns true if VGG16 detects a dog in the image\n",
        "def dog_detector(pic_path):\n",
        "  return((False, True) [151 <= int(VGG16_predict(pic_path)) <= 268])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHp1YxI9JdES",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Assess the Dog Detector\n",
        "\n",
        "__Question 2:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
        "- What percentage of the images in `human_files_short` have a detected dog?\n",
        "- What percentage of the images in `dog_files_short` have a detected dog?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldlHLDNwJdES",
        "colab_type": "text"
      },
      "source": [
        "__Answer:__ \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c9m1DFdJdET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a function called detect, that takes a set of images as input,\n",
        "# and returns the count of how many contained a dog\n",
        "def detect(images):\n",
        "  count = 0\n",
        "  for i in tqdm(images):\n",
        "    if dog_detector(i):\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "# run the detect function on human_pix_short and dog_pix_short, and print the results\n",
        "print('\\nDog(s) detected in the first 100 images from: \\nhuman_pix: {} \\tdog_pix: {}'.format(\n",
        "  detect(human_pix_short), detect(dog_pix_short)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC2VtQVJJdEc",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<a id='step3'></a>\n",
        "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
        "\n",
        "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 10%.  In Step 4 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
        "<!-- \n",
        "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have trouble distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
        "\n",
        "Brittany | Welsh Springer Spaniel\n",
        "- | - \n",
        "<img src=\"https://github.com/JamesDBartlett/deep-learning-v2-pytorch/blob/master/project-dog-classification/images/Brittany_02625.jpg?raw=1\" width=\"100\"> | <img src=\"https://github.com/JamesDBartlett/deep-learning-v2-pytorch/blob/master/project-dog-classification/images/Welsh_springer_spaniel_08203.jpg?raw=1\" width=\"200\">\n",
        "\n",
        "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
        "\n",
        "\n",
        "Curly-Coated Retriever | American Water Spaniel\n",
        "- | -\n",
        "<img src=\"https://github.com/JamesDBartlett/deep-learning-v2-pytorch/blob/master/project-dog-classification/images/Curly-coated_retriever_03896.jpg?raw=1\" width=\"200\"> | <img src=\"https://github.com/JamesDBartlett/deep-learning-v2-pytorch/blob/master/project-dog-classification/images/American_water_spaniel_00648.jpg?raw=1\" width=\"200\">\n",
        "\n",
        "\n",
        "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
        "\n",
        "\n",
        "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
        "- | -\n",
        "<img src=\"https://github.com/JamesDBartlett/deep-learning-v2-pytorch/blob/master/project-dog-classification/images/Labrador_retriever_06457.jpg?raw=1\" width=\"150\"> | <img src=\"https://github.com/JamesDBartlett/deep-learning-v2-pytorch/blob/master/project-dog-classification/images/Labrador_retriever_06455.jpg?raw=1\" width=\"240\"> | <img src=\"https://github.com/JamesDBartlett/deep-learning-v2-pytorch/blob/master/project-dog-classification/images/Labrador_retriever_06449.jpg?raw=1\" width=\"220\">\n",
        "\n",
        "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.   -->\n",
        "\n",
        "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun!\n",
        "\n",
        "### (IMPLEMENTATION) Specify Data Loaders for the Dog Dataset\n",
        "\n",
        "Use the code cell below to write three separate [data loaders](http://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for the training, validation, and test datasets of dog images (located at `dogImages/train`, `dogImages/valid`, and `dogImages/test`, respectively).  You may find [this documentation on custom datasets](http://pytorch.org/docs/stable/torchvision/datasets.html) to be a useful resource.  If you are interested in augmenting your training and/or validation data, check out the wide variety of [transforms](http://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transform)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5_M8krkf2nB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Skip from-scratch model?\n",
        "skipScratch = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD13Z3ZlJdEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specify transforms\n",
        "data_transforms = {\n",
        "  'train': transforms.Compose([        \n",
        "      transforms.ColorJitter(brightness = 0.25, contrast = 0.25, saturation = 0.25),\n",
        "      transforms.RandomRotation(23),\n",
        "      transforms.RandomResizedCrop(224),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.RandomVerticalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                            [0.229, 0.224, 0.225])]),\n",
        "  'valid': transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                            [0.229, 0.224, 0.225])]),\n",
        "  'test': transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                            [0.229, 0.224, 0.225])])}\n",
        "\n",
        "# create a single variable for adjusting all of the co-dependent \n",
        "# convolutional values at once, thereby avoiding typos & mismatches\n",
        "volume_knob_init = 32\n",
        "volume_knob = [x * volume_knob_init for x in [1, 2, 4, 8, 16, 32, 64]]\n",
        "\n",
        "# output of Net should be this shape & size\n",
        "net_shape = 7**2\n",
        "net_size = volume_knob[2]\n",
        "\n",
        "# declare image directories\n",
        "dirs = {'train': 'dogImages' + ddl + 'dogImages' + ddl + 'train', \n",
        "        'valid': 'dogImages' + ddl + 'dogImages' + ddl + 'valid',\n",
        "        'test': 'dogImages' + ddl + 'dogImages' + ddl + 'test'}\n",
        "\n",
        "# create training, validation, and test datasets\n",
        "image_datasets = {x: datasets.ImageFolder(dirs[x], \n",
        "  transform = data_transforms[x]) for x in ['train', 'valid', 'test']}\n",
        "\n",
        "# create dataloaders for training, validation, and test datasets\n",
        "loaders_scratch = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
        "  batch_size = net_size, shuffle = True) for x in ['train', 'valid', 'test']}\n",
        "\n",
        "# store the sizes of training, validation, and test datasets in dataset_sizes variable\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid', 'test']}\n",
        "\n",
        "# extract the image class names from the training dataset\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "# count the number of classes in the training dataset\n",
        "num_classes = len(image_datasets['train'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs7pRP76JdEh",
        "colab_type": "text"
      },
      "source": [
        "**Question 3:** Describe your chosen procedure for preprocessing the data. \n",
        "- How does your code resize the images (by cropping, stretching, etc)?  What size did you pick for the input tensor, and why?\n",
        "- Did you decide to augment the dataset?  If so, how (through translations, flips, rotations, etc)?  If not, why not?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_uJaZeeJdEi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "My code resizes the images by a combination of resizing & cropping. I chose 224 for the input tensor size because it is large enough to retain significantly more detail than the usual thumbnail-size images (like CIFAR or MNIST), but is still small enough that training the model doesn't take forever. It's also the dimensions that many of the pretrained models use, so this helps ensure we're comparing apples to apples. If I had unlimited computing power, I would much prefer my input tensors to be sized to the average dimensions of all images in the dataset, but that's not realistic, so I struck a compromise.\n",
        "\n",
        "- For the training images, I added:\n",
        " 1. 25% jitter on brightness, contrast, and saturation\n",
        " 2. 23° random rotation\n",
        " 3. 224px random resized crop\n",
        " 4. random horizontal & vertical flips\n",
        " 5. conversion to tensor\n",
        " 6. normalization using the default values\n",
        "\n",
        "- For the validation images, I added:\n",
        " 1. 256px resize\n",
        " 2. 224px center crop\n",
        " 3. conversion to tensor\n",
        " 4. normalization using the default values\n",
        "\n",
        "- For the testing images, I added:\n",
        " 1. 256px resize\n",
        " 2. 224px center crop\n",
        " 3. conversion to tensor\n",
        " 4. normalization using the default values\n",
        " \n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5zVrlUbJdEj",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Model Architecture\n",
        "\n",
        "Create a CNN to classify dog breed.  Use the template in the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcN50vsZJdEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# declare \"Net\" class, of type \"nn.Module\"\n",
        "class Net(nn.Module):\n",
        "  \n",
        "  # define model architecture\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    # four convolution layers\n",
        "    self.conv1 = nn.Conv2d(3, volume_knob[0], 3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(volume_knob[0], volume_knob[1], 3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(volume_knob[1], volume_knob[2], 3, padding=1)\n",
        "    self.conv4 = nn.Conv2d(volume_knob[2], volume_knob[2], 3, padding=1)\n",
        "\n",
        "    # three fully-connected layers\n",
        "    self.fc1 = nn.Linear(net_shape * volume_knob[6], volume_knob[4])\n",
        "    self.fc2 = nn.Linear(volume_knob[4], volume_knob[4])\n",
        "    self.fc3 = nn.Linear(volume_knob[4], num_classes) \n",
        "  \n",
        "    # maxpool (with 2 x 2 geometry)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    # dropouts (with 22%, 33%, or 44% chance)\n",
        "    self.dropout22 = nn.Dropout(0.22)\n",
        "    self.dropout33 = nn.Dropout(0.33)\n",
        "    self.dropout44 = nn.Dropout(0.44)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # run relu activation function on all four convolution layers, \n",
        "    # with a pooling step after each one, followed by a dropout\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv4(x))\n",
        "    x = self.dropout33(x)\n",
        "\n",
        "    # flatten convolution output before fully-connected layers\n",
        "    x = x.view(-1, net_shape * volume_knob[6])\n",
        "\n",
        "    # run relu activation function on the first two fully-connected layers,\n",
        "    # with a dropout step after each one, then return the third fully-connected layer\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout44(x)\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.dropout22(x)\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "# instantiate the model\n",
        "model_scratch = Net()\n",
        "\n",
        "# print the model architecture\n",
        "print(model_scratch)\n",
        "\n",
        "# move model to cuda, if available\n",
        "if use_cuda:\n",
        "  model_scratch.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jUJv1ONJdEp",
        "colab_type": "text"
      },
      "source": [
        "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL31Jv7RJdEq",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "__Answer:__  \n",
        "1. Created a variable array called `volume_knob[]` to take the place of all the numerical settings in the model architecture, so that I could make the necessary adjustments in one place, and have the values cascade all the way down through the model. \n",
        "2. Added 4 convolutional layers, because 3 didn't seem like quite enough, and 5 would have been too many!  \n",
        "3. Added 3 fully-connected layers, because it felt like a good number, and the tests that I performed with the model seemed to confirm that idea.\n",
        "4. Added a bit of healthy chaos to the dropout steps, with 3 distinct dropout functions -- each having a different chance of causing a dropout when called.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmLhRC_OJdEr",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
        "\n",
        "Use the next code cell to specify a [loss function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [optimizer](http://pytorch.org/docs/stable/optim.html).  Save the chosen loss function as `criterion_scratch`, and the optimizer as `optimizer_scratch` below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFpyXNccJdEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select the CrossEntropy loss function\n",
        "criterion_scratch = nn.CrossEntropyLoss()\n",
        "\n",
        "# select the Adam optimizer with the amsgrad option set to True\n",
        "optimizer_scratch = optim.Adam(model_scratch.parameters(), lr = 1e-3, amsgrad = True)\n",
        "\n",
        "# reduce learning rate after 2 epochs of non-decreasing validation loss\n",
        "scheduler_scratch = lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer_scratch, mode = 'min', \n",
        "    patience = 2, factor = 1e-2, min_lr = 1e-4\n",
        "    )\n",
        "\n",
        "if use_cuda:\n",
        "  criterion_scratch = criterion_scratch.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEzYXLSjJdEy",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Train and Validate the Model\n",
        "\n",
        "Train and validate your model in the code cell below.  [Save the final model parameters](http://pytorch.org/docs/master/notes/serialization.html) at filepath `'model_scratch.pth'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Dt2OV-JJdEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# declare two empty dictionaries to keep track of the losses and accuracies\n",
        "# of the training, validation, and test datasets, across all epochs.\n",
        "all_losses = {'epoch': [], 'train': [], 'valid': [], 'test': []}\n",
        "all_accuracies = {'epoch': [], 'train': [], 'valid': [], 'test': []}\n",
        "\n",
        "# define the train function\n",
        "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
        "  \n",
        "  # initialize validation loss as infinity\n",
        "  valid_loss_min = np.Inf \n",
        "\n",
        "  # print current time\n",
        "  current_time() \n",
        "  \n",
        "  # define the steps for each epoch\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    all_losses['epoch'].append(epoch)\n",
        "    all_accuracies['epoch'].append(epoch)\n",
        "    start_time = time.time()\n",
        "    horizBar('-', 84)\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "    \n",
        "    # initialize training and validation loss variables with floating-point zeros\n",
        "    train_loss, valid_loss, train_running_corrects, valid_running_corrects = floatingZeros(4)\n",
        "    \n",
        "    # train the model\n",
        "    model.train()\n",
        "\n",
        "    # perform the following actions on the training dataloader's inputs & target\n",
        "    for batch_idx, (data, target) in enumerate(loaders_scratch['train']):\n",
        "\n",
        "      # use cuda, if available\n",
        "      if use_cuda:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "                \n",
        "      # zero out gradients\n",
        "      optimizer_scratch.zero_grad()\n",
        "\n",
        "      # pass inputs to model, get predictions (forward pass)\n",
        "      outputs = model_scratch(data)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "\n",
        "      # calculate loss\n",
        "      loss = criterion_scratch(outputs, target)\n",
        "\n",
        "      # backward pass\n",
        "      loss.backward()\n",
        "\n",
        "      # run the optimizer to update the parameters\n",
        "      optimizer_scratch.step()\n",
        "\n",
        "      # update loss & running corrects \n",
        "      train_loss += data.size(0) * loss.item()\n",
        "      train_running_corrects += torch.sum(preds == target.data)   \n",
        "  \n",
        "    # evaluate the model\n",
        "    model.eval()\n",
        "\n",
        "    # perform the following actions on the validation dataloader's inputs & target\n",
        "    for batch_idx, (data, target) in enumerate(loaders_scratch['valid']):\n",
        "\n",
        "      # move to cuda, if available\n",
        "      if use_cuda:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "\n",
        "      # pass inputs to model, get predictions (forward pass)\n",
        "      outputs = model_scratch(data)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "\n",
        "      # calculate loss\n",
        "      loss = criterion_scratch(outputs, target)\n",
        "\n",
        "      # update loss & running corrects variables\n",
        "      valid_loss += data.size(0) * loss.item()\n",
        "      valid_running_corrects += torch.sum(preds == target.data)\n",
        "        \n",
        "    # calculate and store average accuracies and losses for the training & validation datasets\n",
        "    train_acc = train_running_corrects.double() / len(loaders_scratch['train'].dataset)\n",
        "    all_accuracies['train'].append(train_acc)\n",
        "    valid_acc = valid_running_corrects.double() / len(loaders_scratch['valid'].dataset)\n",
        "    all_accuracies['valid'].append(valid_acc)\n",
        "    train_loss /= len(loaders_scratch['train'].dataset)\n",
        "    all_losses['train'].append(train_loss)\n",
        "    valid_loss /= len(loaders_scratch['valid'].dataset)\n",
        "    all_losses['valid'].append(valid_loss)\n",
        "\n",
        "    # update time it took the epoch to run\n",
        "    time_epoch = time.time() - start_time\n",
        "        \n",
        "    # print results\n",
        "    printLosses(train_loss, valid_loss)\n",
        "    \n",
        "    # if validation loss decreased, save model & update valid_loss_min variable\n",
        "    valid_loss_min = validLossEval(valid_loss_min, valid_loss, model_scratch, 'model_scratch.pth')\n",
        "\n",
        "    # print time it took the epoch to run\n",
        "    printEpochTime(time_epoch)\n",
        "\n",
        "    horizBar('-', 84)     \n",
        "\n",
        "    # Print the current time every 3rd epoch\n",
        "    if epoch % 3 == 0:\n",
        "      current_time() \n",
        "          \n",
        "  return model\n",
        "\n",
        "# move model to cuda, if available\n",
        "if use_cuda:\n",
        "  model_scratch = model_scratch.cuda()\n",
        "  \n",
        "# skip training if skipScratch is True\n",
        "if not (skipScratch):\n",
        "\n",
        "  # Skip training if the model file already exists\n",
        "  if not (path.exists('model_scratch.pth')):\n",
        "\n",
        "    # train model\n",
        "    model_scratch = train(30, loaders_scratch, model_scratch, optimizer_scratch, \n",
        "                          criterion_scratch, use_cuda, 'model_scratch.pth')\n",
        "  # load model with highest accuracy\n",
        "  model_scratch.load_state_dict(torch.load('model_scratch.pth'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2qT5QY0JdE2",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Test the Model\n",
        "\n",
        "Try out your model on the test dataset of dog images.  Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 10%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuLyCPD-JdE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define test function\n",
        "def test(loaders, model, criterion, use_cuda):\n",
        "\n",
        "  # initialize testing loss variables with floating-point zeros\n",
        "  test_loss, correct, total = floatingZeros(3)\n",
        "\n",
        "  # evaluate the model\n",
        "  model.eval()\n",
        "\n",
        "  # perform the following actions on the testing dataloader's inputs & target\n",
        "  for batch_idx, (data, target) in enumerate(loaders['test']):\n",
        "\n",
        "    # move to cuda, if available\n",
        "    if use_cuda:\n",
        "      data, target = data.cuda(), target.cuda()\n",
        "\n",
        "    # pass data to model, get output (forward pass)\n",
        "    output = model(data)\n",
        "\n",
        "    # calculate loss\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    # update test loss variable\n",
        "    test_loss += (loss.data - test_loss) * (1 / (batch_idx + 1))\n",
        "\n",
        "    # predict class from output\n",
        "    pred = output.data.max(1, keepdim = True)[1]\n",
        "\n",
        "    # check predicted class against actual class\n",
        "    correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
        "    total += data.size(0)\n",
        "\n",
        "  # print results        \n",
        "  printTestResults(test_loss, correct, total)\n",
        "\n",
        "\n",
        "# skip testing if skipScratch is True\n",
        "if not (skipScratch):\n",
        "\n",
        "  # call test function    \n",
        "  test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia1ph_KnJdE5",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<a id='step4'></a>\n",
        "## Step 4: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
        "\n",
        "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
        "\n",
        "### (IMPLEMENTATION) Specify Data Loaders for the Dog Dataset\n",
        "\n",
        "Use the code cell below to write three separate [data loaders](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader) for the training, validation, and test datasets of dog images (located at `dogImages/train`, `dogImages/valid`, and `dogImages/test`, respectively). \n",
        "\n",
        "If you like, **you are welcome to use the same data loaders from the previous step**, when you created a CNN from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eagy_BekJdE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# declare image directories\n",
        "dirs = {'train': 'dogImages' + ddl + 'dogImages' + ddl + 'train', \n",
        "        'valid': 'dogImages' + ddl + 'dogImages' + ddl + 'valid',\n",
        "        'test': 'dogImages' + ddl + 'dogImages' + ddl + 'test'}\n",
        "        \n",
        "# specify transforms\n",
        "data_transforms = {\n",
        "  'train': transforms.Compose([        \n",
        "      transforms.ColorJitter(brightness = 0.25, contrast = 0.25, saturation = 0.25),\n",
        "      transforms.RandomRotation(15), transforms.RandomResizedCrop(224),\n",
        "      transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(), transforms.ToTensor(), \n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "  'valid': transforms.Compose([\n",
        "      transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(),\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "  'test': transforms.Compose([\n",
        "      transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(),\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n",
        "\n",
        "# create training, validation, and test datasets\n",
        "image_datasets = {x: datasets.ImageFolder(dirs[x], \n",
        "  transform = data_transforms[x]) for x in ['train', 'valid', 'test']}\n",
        "\n",
        "# create dataloaders for training, validation, and test datasets\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
        "  batch_size = 64, shuffle = True) for x in ['train', 'valid', 'test']}\n",
        "\n",
        "# store the sizes of training, validation, and test datasets in dataset_sizes variable\n",
        "dataset_sizes = {x: len(image_datasets[x]) \n",
        "  for x in ['train', 'valid', 'test']}\n",
        "    \n",
        "# extract the image class names from the training dataset    \n",
        "class_names = image_datasets['train'].classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3OaAmnpJdE9",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Model Architecture\n",
        "\n",
        "Use transfer learning to create a CNN to classify dog breed.  Use the code cell below, and save your initialized model as the variable `model_transfer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv_41YwEJdE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize model\n",
        "model_transfer = densenet161            \n",
        "# model_transfer = wide_resnet50        \n",
        "# model_transfer = wide_resnet101       \n",
        "# model_transfer = resnext101\n",
        "\n",
        "# move model to cuda, if available\n",
        "if use_cuda:\n",
        "  model_transfer = model_transfer.cuda()\n",
        "\n",
        "# declare classifier as nn.Sequential object, containing an OrderedDict of layers to use in the model\n",
        "classifier = nn.Sequential(\n",
        "  OrderedDict([('fc1',\n",
        "                nn.Linear(model_transfer.classifier.in_features,\n",
        "                          model_transfer.classifier.out_features)),\n",
        "              ('relu', nn.ReLU()), \n",
        "              ('bn', \n",
        "                nn.BatchNorm1d(model_transfer.classifier.out_features)),                        \n",
        "              ('fc2',\n",
        "                nn.Linear(model_transfer.classifier.out_features,\n",
        "                         len(class_names))),\n",
        "              ('relu', nn.ReLU()), \n",
        "              ('output', nn.LogSoftmax(dim = 1))]))\n",
        "\n",
        "# turn off training on feature layers\n",
        "for param in model_transfer.features.parameters():\n",
        "  param.requires_grad = False\n",
        "  \n",
        "# set model_transfer's classifier to previously-declared classifier object\n",
        "model_transfer.classifier = classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXouZ4sXJdFD",
        "colab_type": "text"
      },
      "source": [
        "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceIGIOw6JdFE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "__Answer:__ \n",
        "\n",
        "\n",
        "1. I used the same image_datasets, dataloaders, dataset_sizes, class_names, and data_transforms from the previous model, to keep the comparison between them as fair as possible.\n",
        "2. I decided to use DenseNet for my pretrained model because it's fast, and also because it was trained on ImageNet, which makes it a good starting place for image classification problems like this.\n",
        "3. I replaced the model's pretrained classification layers with the following: \n",
        " - 1 fully-connected layer as the input\n",
        " - 1 relu activation function\n",
        " - 1 BatchNorm1d layer\n",
        " - 1 fully-connected layer\n",
        " - 1 relu activation function\n",
        " - 1 LogSoftMax function as the output\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9WGodusJdFE",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
        "\n",
        "Use the next code cell to specify a [loss function](http://pytorch.org/docs/master/nn.html#loss-functions) and [optimizer](http://pytorch.org/docs/master/optim.html).  Save the chosen loss function as `criterion_transfer`, and the optimizer as `optimizer_transfer` below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx4JVAOxJdFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select the CrossEntropy loss function\n",
        "criterion_transfer = nn.CrossEntropyLoss()\n",
        "\n",
        "# select the Adam optimizer with the amsgrad option set to True\n",
        "optimizer_transfer = optim.Adam(model_transfer.classifier.parameters(), lr = 1e-3, amsgrad = True)\n",
        "\n",
        "# reduce learning rate after 2 epochs of non-decreasing validation loss\n",
        "scheduler_transfer = lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer_transfer, mode = 'min', \n",
        "    patience = 2, factor = 1e-2, min_lr = 1e-4\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUwa9ymgJdFI",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Train and Validate the Model\n",
        "\n",
        "Train and validate your model in the code cell below.  [Save the final model parameters](http://pytorch.org/docs/master/notes/serialization.html) at filepath `'model_transfer.pt'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRMqmooHJdFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# declare two empty dictionaries to keep track of the losses and accuracies\n",
        "# of the training, validation, and test datasets, across all epochs.\n",
        "all_losses = {'epoch': [], 'train': [], 'valid': [], 'test': []}\n",
        "all_accuracies = {'epoch': [], 'train': [], 'valid': [], 'test': []}\n",
        "\n",
        "# move model to cuda, if available\n",
        "if use_cuda:\n",
        "  model_transfer = model_transfer.cuda()\n",
        "\n",
        "# Skip training if the model file already exists\n",
        "if not (path.exists('model_transfer.pth')):\n",
        "\n",
        "  # set number of training epochs to 10\n",
        "  n_epochs = 10\n",
        "\n",
        "  # initialize validation loss as infinity\n",
        "  valid_loss_min = np.Inf\n",
        "\n",
        "  # print the current time\n",
        "  current_time()\n",
        "\n",
        "  # define the steps for each epoch\n",
        "  for epoch in range(1, n_epochs + 1):    \n",
        "    all_losses['epoch'].append(epoch)\n",
        "    all_accuracies['epoch'].append(epoch)\n",
        "    start_time = time.time()\n",
        "    horizBar('-', 84)\n",
        "    print('Epoch: {}'.format(epoch))\n",
        "\n",
        "    # initialize training and validation loss variables with floating-point zeros\n",
        "    train_loss, valid_loss, train_running_corrects, valid_running_corrects = floatingZeros(4)\n",
        "    \n",
        "    # train the model \n",
        "    model_transfer.train()\n",
        "\n",
        "    # perform the following actions on the training dataloader's inputs & target\n",
        "    for inputs, target in dataloaders['train']:\n",
        "\n",
        "      # move to cuda, if available\n",
        "      inputs, target = inputs.cuda(), target.cuda()\n",
        "\n",
        "      # zero out gradients\n",
        "      optimizer_transfer.zero_grad()\n",
        "\n",
        "      # pass inputs to model, get predictions (forward pass)\n",
        "      outputs = model_transfer(inputs)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "\n",
        "      # calculate loss\n",
        "      loss = criterion_transfer(outputs, target)\n",
        "\n",
        "      # backward pass\n",
        "      loss.backward()\n",
        "\n",
        "      # run the optimizer to update the parameters\n",
        "      optimizer_transfer.step()\n",
        "\n",
        "      # update loss & running corrects variables\n",
        "      train_loss += inputs.size(0) * loss.item()\n",
        "      train_running_corrects += torch.sum(preds == target.data)        \n",
        "        \n",
        "    # evaluate the model\n",
        "    model_transfer.eval()\n",
        "\n",
        "    # perform the following actions on the validation dataloader's inputs & target\n",
        "    for inputs, target in dataloaders['valid']:\n",
        "\n",
        "      # move to cuda if available\n",
        "      if use_cuda:\n",
        "        inputs, target = inputs.cuda(), target.cuda()\n",
        "\n",
        "      # pass inputs to model, get predictions (forward pass)\n",
        "      outputs = model_transfer(inputs)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "\n",
        "      # calculate loss\n",
        "      loss = criterion_transfer(outputs, target)\n",
        "\n",
        "      # update loss & running corrects variables\n",
        "      valid_loss += inputs.size(0) * loss.item()\n",
        "      valid_running_corrects += torch.sum(preds == target.data)\n",
        "    \n",
        "    # calculate and store average accuracies and losses for the training & validation datasets\n",
        "    train_acc = train_running_corrects.double() / len(dataloaders['train'].dataset)\n",
        "    all_accuracies['train'].append(train_acc)\n",
        "    valid_acc = valid_running_corrects.double() / len(dataloaders['valid'].dataset)\n",
        "    all_accuracies['valid'].append(valid_acc)\n",
        "    train_loss /= len(dataloaders['train'].dataset)\n",
        "    all_losses['train'].append(train_loss)\n",
        "    valid_loss /= len(dataloaders['valid'].dataset)\n",
        "    all_losses['valid'].append(valid_loss)\n",
        "    \n",
        "    # calculate the time it took the epoch to run\n",
        "    time_epoch = time.time() - start_time\n",
        "        \n",
        "    # print losses & accuracies\n",
        "    printLosses(train_loss, valid_loss)\n",
        "    printAccuracies(train_acc, valid_acc)\n",
        "\n",
        "    # if validation loss decreased, save model & update valid_loss_min variable      \n",
        "    valid_loss_min = validLossEval(valid_loss_min, valid_loss, model_transfer, 'model_transfer.pth')\n",
        "\n",
        "    # print the time it took the epoch to run\n",
        "    printEpochTime(time_epoch)\n",
        "\n",
        "    horizBar('-', 84) \n",
        "\n",
        "    # Print the current time every other epoch\n",
        "    if epoch % 2 == 0:\n",
        "      current_time()\n",
        "\n",
        "# load model with highest accuracy\n",
        "model_transfer.load_state_dict(torch.load('model_transfer.pth'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLR8--EzJdFM",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Test the Model\n",
        "\n",
        "Try out your model on the test dataset of dog images. Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 60%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abEEu_gvJdFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize testing loss and running corrects variables with floating-point zeros\n",
        "test_loss, test_running_corrects = floatingZeros(2)\n",
        "\n",
        "# test the model\n",
        "model_transfer.eval()\n",
        "\n",
        "# perform the following actions on the testing dataloader's inputs & target\n",
        "for inputs, target in dataloaders['test']:\n",
        "\n",
        "  # move to cuda, if available\n",
        "  if use_cuda:\n",
        "    inputs, target = inputs.cuda(), target.cuda()\n",
        "\n",
        "  # pass inputs to model, get predictions (forward pass)\n",
        "  outputs = model_transfer(inputs)\n",
        "  _, preds = torch.max(outputs, 1)\n",
        "\n",
        "  # calculate loss\n",
        "  loss = criterion_transfer(outputs, target)\n",
        "\n",
        "  # update loss & running corrects variables \n",
        "  test_loss += inputs.size(0) * loss.item()\n",
        "  test_running_corrects += torch.sum(preds == target.data)\n",
        "\n",
        "# calculate and store average accuracies and losses for the testing dataset\n",
        "test_acc = test_running_corrects.double() / len(dataloaders['test'].dataset)\n",
        "all_accuracies['test'].append(test_acc)\n",
        "test_loss /= len(dataloaders['test'].dataset)\n",
        "all_losses['test'].append(test_loss)\n",
        "\n",
        "# print results\n",
        "print(f\"Test Loss: {test_loss:.6f} \\tTest Accuracy: {(test_acc * 100):.6f}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxHTdxo0JdFP",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
        "\n",
        "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan hound`, etc) that is predicted by your model.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRbHyJ7TJdFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# replace underscores in class_names with spaces, and store the results back into class_names\n",
        "class_names = [item[4:].replace(\"_\", \" \") for item in class_names]\n",
        "\n",
        "# define predict_breed_transfer function, which takes a single image as its argument\n",
        "def predict_breed_transfer(pic_path):\n",
        "  \n",
        "  # pre-process the input image\n",
        "  pic = preProcessPic(pic_path) \n",
        "\n",
        "  # convert it to a tensor\n",
        "  pic = torch.from_numpy(pic).type(torch.FloatTensor) \n",
        "  pic.unsqueeze_(0)\n",
        "\n",
        "  # move the tensor to cuda, if available\n",
        "  if use_cuda:\n",
        "    pic = pic.cuda()\n",
        "\n",
        "  # predict image label\n",
        "  probs = torch.exp(model_transfer.forward(pic)) \n",
        "  _, label = probs.topk(1)     \n",
        "  return class_names[int(label)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJcVZKxsJdFS",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<a id='step5'></a>\n",
        "## Step 5: Write your Algorithm\n",
        "\n",
        "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
        "- if a __dog__ is detected in the image, return the predicted breed.\n",
        "- if a __human__ is detected in the image, return the resembling dog breed.\n",
        "- if __neither__ is detected in the image, provide output that indicates an error.\n",
        "\n",
        "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `dog_detector` functions developed above.  You are __required__ to use your CNN from Step 4 to predict dog breed.  \n",
        "\n",
        "<!-- Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
        "\n",
        "![Sample Human Output](https://github.com/JamesDBartlett/deep-learning-v2-pytorch/blob/master/project-dog-classification/images/sample_human_output.png?raw=1) -->\n",
        "\n",
        "\n",
        "### (IMPLEMENTATION) Write your Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5PIaZ98JdFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define run_app function, which takes a single image as its argument\n",
        "def run_app(pic_path):\n",
        "\n",
        "  # display the input image\n",
        "  display.display(display.Image(filename = pic_path, width = 256))\n",
        "  \n",
        "  # if subject is human, indicate so, then say what dog breed they look most like\n",
        "  if human_face_detector(pic_path):\n",
        "    print(\"Human detected. Dog breed this human looks most like: \")\n",
        "\n",
        "  # if subject is a dog, indicate so, then predict its breed\n",
        "  elif dog_detector(pic_path):\n",
        "    print(\"Dog detected. Most likely breed: \")\n",
        "  \n",
        "  # otherwise, indicate that no humans or dogs were found, and return null\n",
        "  else:\n",
        "    print(\"Humans and/or dogs found in this photo:\")\n",
        "    return\n",
        "\n",
        "  # return predicted dog breed of the detected human or dog\n",
        "  return predict_breed_transfer(pic_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uen1zKE4JdFW",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<a id='step6'></a>\n",
        "## Step 6: Test Your Algorithm\n",
        "\n",
        "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that _you_ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
        "\n",
        "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
        "\n",
        "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
        "\n",
        "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm0FWM49JdFW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "__Answer:__  \n",
        "\n",
        "\n",
        "I thought my model performed very well, though it could certainly be improved. Here are 3 ways I might improve it in the future:\n",
        "1. Increase the number of training epochs until the model's prediction accuracy comes to a plateau.\n",
        "2. Experiment with using other pretrained models, like resnext101_32x8d, or one of the wide_resnet models.\n",
        "3. Experiment with adding, removing, or changing the fully-connected layers and the output layer.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdWGNiPBJjxQ",
        "colab_type": "text"
      },
      "source": [
        "### Test using images from the original (udacity-aind/dog-project) datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGrGS99SJdFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "horizBar('*', 84)\n",
        "\n",
        "print(\"\\nAnalyzing images from human_pix and dog_pix datasets...\\n\")\n",
        "\n",
        "# pick 10 random images from each of the human_pix and dog_pix datasets\n",
        "random_humans = random.sample(list(human_pix), 10)\n",
        "random_dogs = random.sample(list(dog_pix), 10)\n",
        "\n",
        "# call run_app function on the contents of the random_humans and random_dogs arrays\n",
        "for file in np.hstack((random_humans[:10], random_dogs[:10])):\n",
        "  horizBar('-', 84)\n",
        "  print(\" - \" + run_app(file) + \"\\n\")\n",
        "  horizBar('-', 84)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAHlDnjEJ3LL",
        "colab_type": "text"
      },
      "source": [
        "### Test using trending images from the following categories on [stocksnap.io](https://stocksnap.io):\n",
        "\n",
        "*   dog\n",
        "*   woman face\n",
        "*   man face\n",
        "*   fruit\n",
        "*   car\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS5Zjdr-Hgc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"\\nDownloading and analyzing images from custom dataset (stocksnap.io)...\\n\")\n",
        "\n",
        "# declare variable for test images folder name\n",
        "test_images = 'test-images'\n",
        "\n",
        "# create test-images folder, if it doesn't already exist\n",
        "if not (path.exists(test_images)):\n",
        "  os.mkdir(test_images)\n",
        "\n",
        "# from stocksnap.io, download 5 random images from each of these 5 categories: \n",
        "## dog, woman face, man face, fruit, car \n",
        "# save the images in the test-images folder, and log any errors to test-images-download-log.txt\n",
        "!image-scraper -m 5 -s \"test-images\" \"https://stocksnap.io/search/dog/sort/trending/desc\" \\\n",
        "  --formats jpg --scrape-reverse >> test-images-download-log.txt\n",
        "!image-scraper -m 5 -s \"test-images\" \"https://stocksnap.io/search/woman+face/sort/trending/desc\" \\\n",
        "  --formats jpg --scrape-reverse >> test-images-download-log.txt\n",
        "!image-scraper -m 5 -s \"test-images\" \"https://stocksnap.io/search/man+face/sort/trending/desc\" \\\n",
        "  --formats jpg --scrape-reverse >> test-images-download-log.txt\n",
        "!image-scraper -m 5 -s \"test-images\" \"https://stocksnap.io/search/fruit/sort/trending/desc\"  \\\n",
        "  --formats jpg --scrape-reverse >> test-images-download-log.txt\n",
        "!image-scraper -m 5 -s \"test-images\" \"https://stocksnap.io/search/car/sort/trending/desc\"  \\\n",
        "  --formats jpg --scrape-reverse >> test-images-download-log.txt\n",
        "\n",
        "# create list of all jpg images in test-images folder\n",
        "testImageList = [f for f in os.listdir(test_images) if f.endswith('.jpg')]\n",
        "\n",
        "print()\n",
        "horizBar('*', 90)\n",
        "print(\"\\tNOTE:\")\n",
        "print(f\"\\tThere are currently {len(testImageList):0d} images in the testImageList dataset.\")\n",
        "print(\"\\tTo download more images and add them to the dataset, run this cell again.\")\n",
        "horizBar('*', 90)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYHKmWx1MEna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pick 25 images at random from testImageList\n",
        "testImageList = random.sample(list(testImageList), 25)\n",
        "\n",
        "# iterate over the image list\n",
        "for i in testImageList:\n",
        "\n",
        "  horizBar('-', 84)\n",
        "\n",
        "  # call run_app function, pass current image as argument, print output\n",
        "  print(\" - \" + run_app(test_images + ddl + i) + \"\\n\")\n",
        "\n",
        "  horizBar('-', 84)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}